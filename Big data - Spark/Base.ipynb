{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Man! We have reached to BIG fucking DATA! (in a nice way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"BigData - BigPicture.png\" width= 1000 height= 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"nexa\" size= 7> Main Idea </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have worked on the scale of such data, which can be fit in our RAM - 0~8.\n",
    "* What if we have the large data...\n",
    "    - SQL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to do `BAYOND` what out machines can do - we can work with <u> **Distributed** system </u> which distrubutes the data to multiple machines or the computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"nexa\" size= 5> In Local </font><br>\n",
    "We need to rely on the RAM or HDD\n",
    "___\n",
    "<font face=\"nexa\" size= 5> In Distributed </font><br>\n",
    "We have ONE machine, controling the distribution to other machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <font face= \"eb garamond\" size= 4> <i> \" After a certain point, it is easier to scale out to many <br> LOWER CPU, than to try to scale up to a single machine with HIGH CPU \" </i> </font> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"nexa\" style= \"font-size:100px\"> HADOOP </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is <u>a way</u> `to distribute` very large files across multiple machines\n",
    "* It uses Hadoop Distributed File System (HDFS)\n",
    "* Allows for Large Data\n",
    "* Keeps backups for \"Fault Tolerance\" (If one machine failes, other not) <br>\n",
    "_____ <br>\n",
    "* MapReduce is used by Hadoop.\n",
    "    - MapReduce allows Computation on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"HDFS.png\" height= 1000 width= 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MapReduce.png\" height= 1000 width= 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS:\n",
    "> To distribute large datasets\n",
    "## MapReduce:\n",
    "> To distribute a computational task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"nexa\" style= \"font-size:100px\"> SPARK </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <u>One of the latest technologies</u> being used to quickly and easily <u>handle Big Data</u>\n",
    "* Open source project on Apache\n",
    "* Created at AMPLab at UC Berkeley\n",
    "> Spark as a Flexible alternative to MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It can use data in VARIETY of formats:\n",
    "* HDFS (Hadoop wala)\n",
    "* AWS S3\n",
    "* Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We should see spark as an alternative to MapReduce than Hadoop  \n",
    "\n",
    "as\n",
    "\n",
    "> it privides comprehansive solutions to handle big data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What makes *MapReduce* slower than *Spark*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MapReduce requires files to be stored in HDFS **`ONLY`**.\n",
    "* Spark is 100x faster.\n",
    "\n",
    "### Why?\n",
    "___\n",
    "MAP: Because MapReduce writes most of the <u>data to disk(HDD)</u> after each **Map & Reduce operation.**  \n",
    "SPARK: It keeps most of the <u>data in memory(RAM)</u> after each transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `\"RDD\": Resilient Distributed Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    So till now... the picture is...\n",
    "    \n",
    "                                                    \n",
    "                                                    \n",
    "                                                    HADOOP\n",
    "                                                      |\n",
    "                                                     / \\\n",
    "                                                    /   \\\n",
    "                                             MapReduce   Spark\n",
    "                                                |          |  \n",
    "                                                _         RDD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What and Why RDD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distributed Collection of Data\n",
    "* Fault-tolerant\n",
    "* Parallel operation\n",
    "* Ability to use many data sources\n",
    "\n",
    "_____\n",
    "\n",
    "RDDs are: \n",
    "- Immutable\n",
    "- Lazily evaluated\n",
    "- Cashable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RDD.png\" width= 1000 height= 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAG: Directed Asynctic Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Main operations with RDD:** `Transformations & Actions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "- **Filter** - Apply function to each element and return those True (same as python)\n",
    "- **Map** - Transforms each element and preserves same element (same as python)\n",
    "- **Flatmap** - It will change number of elements from 0 to N (NEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "- **First** - Returns *`first`* element of RDD\n",
    "- **Collect** - Returns *`all`* elements of RDD\n",
    "- **Count** - Return *`number of elements`* of RDD\n",
    "- **Take** - Return *`first n`* elements of RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra\n",
    "* **Reduce** - Which will Aggregate <u>RDD</u> elements by func that returns single element\n",
    "* **ReduceByKey** - Which will Aggregate <u>Pair RDD</u> by func that returns pair RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark has a big ecosystem\n",
    "- Spark SQL\n",
    "- Sparf DataFrames\n",
    "- MLlib\n",
    "- GraphX\n",
    "- Spark Streaming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
